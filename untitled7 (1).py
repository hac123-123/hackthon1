# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vd9X4z2qUQ8L5PNYkR8OnCeavBvOSp5a
"""

# Install required packages
!pip install transformers torch gradio accelerate -q

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import gradio as gr

# Load Granite model and tokenizer
# Available options: granite-3.0-2b-instruct, granite-3.0-8b-instruct, granite-3.1-2b-instruct, granite-3.1-8b-instruct
model_name = "ibm-granite/granite-3.0-2b-instruct"  # Using 2B model for faster loading in Colab
print("Loading model...")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

# Set pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def generate_response(prompt, max_length=200, temperature=0.7):
    """Generate AI response using Granite model"""
    try:
        # Tokenize input
        inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)

        # Generate response
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_length=max_length,
                temperature=temperature,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        # Decode response
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Remove the original prompt from response
        if prompt in response:
            response = response.replace(prompt, "").strip()

        return response

    except Exception as e:
        return f"Error generating response: {str(e)}"

# Create Gradio interface
def gradio_interface(user_input, max_tokens, temp):
    """Gradio interface function"""
    if not user_input.strip():
        return "Please enter a prompt."

    response = generate_response(user_input, max_tokens, temp)
    return response

# Build Gradio app
with gr.Blocks(title="Granite AI Assistant") as app:
    gr.Markdown("# ðŸ¤– Granite AI Assistant")
    gr.Markdown("Powered by IBM Granite-3.2-2B Flash model")

    with gr.Row():
        with gr.Column():
            user_input = gr.Textbox(
                label="Your Prompt",
                placeholder="Enter your question or prompt here...",
                lines=3
            )

            with gr.Row():
                max_tokens = gr.Slider(
                    minimum=50,
                    maximum=500,
                    value=200,
                    label="Max Tokens"
                )
                temperature = gr.Slider(
                    minimum=0.1,
                    maximum=1.0,
                    value=0.7,
                    label="Temperature"
                )

            generate_btn = gr.Button("Generate Response", variant="primary")

        with gr.Column():
            output = gr.Textbox(
                label="AI Response",
                placeholder="Response will appear here...",
                lines=10
            )

    # Connect interface
    generate_btn.click(
        fn=gradio_interface,
        inputs=[user_input, max_tokens, temperature],
        outputs=output
    )

    # Example inputs
    gr.Examples(
        examples=[
            ["Write a short story about space exploration"],
            ["Explain quantum computing in simple terms"],
            ["Create a Python function to calculate fibonacci numbers"]
        ],
        inputs=user_input
    )

# Launch the app
print("Starting Gradio app...")
app.launch(share=True, debug=True)